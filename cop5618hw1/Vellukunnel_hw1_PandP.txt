

1. Which STL container, if any, did you use for your thread→value map?
Answer: I used the container, std::unordered_map.

2. Which pattern did you use (fully synchronized object, etc.)?
Answer: I used the Coarse-grained synchronization pattern with an almost fully synchronized object where all the public methods in the class are guarded with the same lock.

3. Consider the following pairs of operations:

a. Set value where the thread does not have a preexisting value.
b. Set value where the thread has a preexisting value
c. Attempt to read a non-existing value
d. Read an existing value
e. Remove a value
f. Attempt to remove a non-existing value

Consider all possible pairs of operations by different threads, and indicate which ones conflict. For convenience in grading, please use the following list.

Answer:

a,a -> CONFLICT!
a,b -> CONFLICT!
a,c -> CONFLICT!
a,d -> CONFLICT!
a,e -> CONFLICT!
a,f -> CONFLICT!
b,b -> CONFLICT!
b,c -> CONFLICT!
b,c -> CONFLICT!
b,d -> CONFLICT!
b,e -> CONFLICT!
b,f -> CONFLICT!
c,c -> No conflict.
c,d -> No conflict.
c,e -> CONFLICT!
c,f -> No conflict.
d,d -> No conflict.
d,e -> CONFLICT!
d,f -> No conflict.
e,e -> CONFLICT!
e,f -> CONFLICT!
f,f -> No conflict.


4. For each of the conflicting operations, in part 2, indicate whether it involves a data race.

Answer: Since I've used an almost fully synchronized object and have used the methods in the class only after obtaining a common lock, there are NO data races in any of the following combination of conflicts.

a,a -> No data race!
a,b -> No data race!
a,c -> No data race!
a,d -> No data race!
a,e -> No data race!
a,f -> No data race!
b,b -> No data race!
b,c -> No data race!
b,c -> No data race!
b,d -> No data race!
b,e -> No data race!
b,f -> No data race!
c,e -> No data race!
d,e -> No data race!
e,e -> No data race!
e,f -> No data race!

5. (COP5618 students only) Consider whether applying some constraints on the way a threadLocal
variable is used (for example: not allowing a set after a remove, or knowing the number of threads in advance, or other possibilities) would allow you to offer a correct implementation with less synchronization than the one you provided. You may change the public interface. If you think so, describe your solution and justify its correctness. If you think that it is not possible to do this justify your answer. No implementation is required, just thinking. 
Hint: Make sure your answer is succinct and clear and use appropriate vocabulary.

Answer: 

The implementation which I have used gives a lot of hotspot and bottleneck, so it is not scalable when there is a lot of threads trying to access the threadLocal variable at once.

One way to offer better finer grained synchronization is to implement a solution for famous readers-writers problem here. We have the set/remove methods, which is similar to a writer, and the get method, which is similar to the reader. Here, the get method is only reading data from the data-structure since we do away with the initialValue method.

So therefore, we can have better synchronization by allowing multiple readers and only one writer at any given time for our threadLocal variable. In C++, this could be achieved by using conditional variables, atomic awaits (in C++ it would be wait_for or wait_until on the 'predicates', which are our conditionals) on these conditionals, and by making sure the operations are atomic on them.

For example, we could have conditional variables nr = number of readers and nw = number of writers. The invariant would be nr ≥ 0 ∧ nw ≥ 0 ∧ (nw == 0 ∨  (nw == 1 and nr == 0)).

We would therefore make our conditional variables atomic, and update the conditional variables as necessarily needed in the invariant within the methods.

get() {
void startGet()
< await(nw =0) nr++ >

void EndGet()
< nr-- >
}

set() {
startWrite
< await(nw == 0 and nr == 0) nw++ >

endWrite
< nw-- >
}

This is better than our earlier implementation because it provides us the additional feature of having multiple threads being able to read their localVariables at the same time (using get method), which wasn't possible before (earlier, each thread had to wait before any other threads finished reading their values before the thread read its own value).

 